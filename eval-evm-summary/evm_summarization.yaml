name: "EVM Summarization Evaluation"
version: "1.0.0"
description: "EVM Semantics Summarization Evaluation Configuration"

metadata:
  author: "Perfx Team"
  date: "2024-12-19"
  tags: ["evm", "semantics", "summarization", "evaluation"]

# Platform condition definitions
# These conditions are used to automatically select the correct build commands based on platform
# - platform.macbook: Apple Silicon Mac (requires special compiler settings)
# - platform.other: Other platforms (uses default settings)
conditions:
  platform.macbook: "platform.system() == 'Darwin' and platform.machine() == 'arm64'"
  platform.other: "platform.system() != 'Darwin' or platform.machine() != 'arm64'"

global:
  working_directory: "."
  output_directory: "results"
  timeout: 18000  # 5 hours
  parallel: false
  verbose: true
  
  # Environment variable settings
  environment:
    K_FRAMEWORK: "llvm"
    PYTHONPATH: "${PYTHONPATH}:./repositories/evm-semantics/kevm-pyk"

# Repository dependencies
repositories:
  - name: "evm-semantics"
    url: "https://github.com/runtimeverification/evm-semantics.git"
    branch: "jh/evaluate-summarize"
    path: "evm-semantics"
    submodules: true

# Evaluation steps
steps:
  # Step 1: Environment preparation
  - name: "setup_environment"
    description: "Set up evaluation environment"
    enabled: true
    commands:
      # Create necessary directories
      - command: "mkdir -p results/backups results/logs results/data"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # MacBook compiler environment setup
      - command: "python eval-evm-summary/setup_macos_env.py"
        cwd: "."
        timeout: 60
        expected_exit_code: 0
        environment:
          APPLE_SILICON: "true"
        condition: "platform.macbook"
      # Skip special setup for non-MacBook platforms
      - command: "echo 'Non-MacBook platform, skipping MacBook-specific setup'"
        cwd: "."
        timeout: 10
        expected_exit_code: 0
        condition: "platform.other"
      # Record environment setup completion
      - command: "echo \"Environment setup completed at $(date)\" > results/logs/setup.log"
        cwd: "."
        timeout: 30
        expected_exit_code: 0

  # Step 2: Build KEVM semantics
  - name: "build_kevm"
    description: "Build KEVM semantics"
    enabled: true
    depends_on: ["setup_environment"]
    commands:
      # Update submodules
      - command: "git submodule update --init --recursive"
        cwd: "repositories/evm-semantics"
        timeout: 600
        expected_exit_code: 0
      # Install dependencies (using uv)
      - command: "uv sync"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 300
        expected_exit_code: 0
      # MacBook build plugin (Apple Silicon)
      - command: "APPLE_SILICON=true uv --project kevm-pyk run -- kdist --verbose build evm-semantics.plugin"
        cwd: "repositories/evm-semantics"
        timeout: 1800  # 30 minutes
        expected_exit_code: 0
        retry:
          max_attempts: 2
          delay: 60
          on_failure: "build_recovery"
        environment:
          CC: "/opt/homebrew/opt/llvm@14/bin/clang"
          CXX: "/opt/homebrew/opt/llvm@14/bin/clang++"
        condition: "platform.macbook"
      # Generic platform build plugin
      - command: "uv --project kevm-pyk run -- kdist --verbose build evm-semantics.plugin"
        cwd: "repositories/evm-semantics"
        timeout: 1800  # 30 minutes
        expected_exit_code: 0
        retry:
          max_attempts: 2
          delay: 60
          on_failure: "build_recovery"
        condition: "platform.other"
      # MacBook build all semantics (Apple Silicon)
      - command: "APPLE_SILICON=true uv --project kevm-pyk run -- kdist --verbose build -j6"
        cwd: "repositories/evm-semantics"
        timeout: 3600  # 1 hour
        expected_exit_code: 0
        retry:
          max_attempts: 2
          delay: 120
          on_failure: "build_recovery"
        environment:
          CC: "/opt/homebrew/opt/llvm@14/bin/clang"
          CXX: "/opt/homebrew/opt/llvm@14/bin/clang++"
        condition: "platform.macbook"
      # Generic platform build all semantics
      - command: "uv --project kevm-pyk run -- kdist --verbose build -j6"
        cwd: "repositories/evm-semantics"
        timeout: 3600  # 1 hour
        expected_exit_code: 0
        retry:
          max_attempts: 2
          delay: 120
          on_failure: "build_recovery"
        condition: "platform.other"
    dependencies:
      - path: "repositories/evm-semantics/kevm-pyk/src/kevm_pyk/kproj/evm-semantics"
        type: "directory"

  # Step 2.5: Build failure recovery
  - name: "build_recovery"
    description: "Recovery handling after build failure"
    enabled: true
    depends_on: ["build_kevm"]
    condition: "on_failure"
    commands:
      # Remove failed build directory
      - command: "rm -rf repositories/evm-semantics"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # Re-execute environment preparation
      - command: "uv run perfx run -c eval-evm-summary/evm_summarization.yaml -s setup_environment"
        cwd: "."
        timeout: 1800  # 30 minutes
        expected_exit_code: 0
      # Re-execute build
      - command: "uv run perfx run -c eval-evm-summary/evm_summarization.yaml -s build_kevm"
        cwd: "."
        timeout: 7200  # 2 hours
        expected_exit_code: 0

  # Step 3: Summarization evaluation
  - name: "summarize_evaluation"
    description: "EVM opcode summarization evaluation"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      # Execute complete summarization validity evaluation
      - command: "uv run python ../../../eval-evm-summary/summarize_evaluator.py --verbose --timeout 1800 --workers 4 --output ../../../results/data/summarize_evaluation_results.json"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 1800  # 30 minutes
        expected_exit_code: 0

  # Step 4: Prove summarization semantics correctness
  - name: "prove_summaries"
    description: "Summarization semantics correctness verification"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      - command: "uv run -- pytest src/tests/integration/test_prove.py::test_prove_summaries --verbose --durations=0 --dist=worksteal --numprocesses=8"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 7200
        expected_exit_code: 0
        outputs:
          - input: stdout
            output: "results/data/prove_summaries_results.txt"
          - input: stdout
            parser: pytest
            output: "results/data/prove_summaries_results.json"
          - input: stderr
            output: "results/data/prove_summaries_errors.txt"
  
  # Step 5: Pure Concrete Execution performance testing
  - name: "pure_concrete_performance"
    description: "Pure Concrete Execution performance testing"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      # Modify to pure semantics mode (llvm-pure)
      - command: "sed -i.bak \"s/kdist.get('evm-semantics.llvm')/kdist.get('evm-semantics.llvm-pure')/g\" repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # Run pure concrete execution performance testing
      - command: "uv run -- pytest src/tests/integration/test_conformance.py --durations=0 --verbose"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 7200
        expected_exit_code: 0
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/pure_concrete_performance.txt"
          - input: stdout
            parser: pytest
            output: "results/data/pure_concrete_performance.json"
          - input: stderr
            output: "results/data/pure_concrete_performance_errors.txt"
      # Cleanup: restore to original state
      - command: "mv repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py.bak repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
        cleanup: true

  # Step 6: Summary Concrete Execution performance testing
  - name: "summary_concrete_performance"
    description: "Summary Concrete Execution performance testing"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      # Modify to summarization semantics mode (llvm-summary)
      - command: "sed -i.bak \"s/kdist.get('evm-semantics.llvm')/kdist.get('evm-semantics.llvm-summary')/g\" repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # Run summary concrete execution performance testing
      - command: "uv run -- pytest src/tests/integration/test_conformance.py --durations=0 --verbose"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 7200
        expected_exit_code: 0
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/summary_concrete_performance.txt"
          - input: stdout
            parser: pytest
            output: "results/data/summary_concrete_performance.json"
          - input: stderr
            output: "results/data/summary_concrete_performance_errors.txt"
      # Cleanup: restore to original state
      - command: "mv repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py.bak repositories/evm-semantics/kevm-pyk/src/kevm_pyk/interpreter.py"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
        cleanup: true

  # Step 7: Pure Symbolic Execution performance testing
  - name: "pure_symbolic_performance"
    description: "Pure Symbolic Execution performance testing"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      # Cleanup: restore original files and remove backup files
      - command: "git checkout -- tests/specs && git clean -f -d tests/specs/"
        cwd: "repositories/evm-semantics"
        timeout: 30
        expected_exit_code: 0
      # Replace EDSL with EDSL-PURE
      - command: "find repositories/evm-semantics/tests/specs -type f -exec sed -i.bak 's/EDSL/EDSL-PURE/g' {} \\;"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # Run test-prove-rules (booster mode)
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_rules --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/pure_symbolic_prove_rules_booster.txt"
          - input: stdout
            parser: pytest
            output: "results/data/pure_symbolic_prove_rules_booster.json"
          - input: stderr
            output: "results/data/pure_symbolic_prove_rules_booster_errors.txt"
      # Run test-prove-rules (booster-dev mode)
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_rules --tb=short --use-booster-dev --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/pure_symbolic_prove_rules_booster_dev.txt"
          - input: stdout
            parser: pytest
            output: "results/data/pure_symbolic_prove_rules_booster_dev.json"
          - input: stderr
            output: "results/data/pure_symbolic_prove_rules_booster_dev_errors.txt"
      # Run test-prove-summaries
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_summaries --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/pure_symbolic_prove_summaries.txt"
          - input: stdout
            parser: pytest
            output: "results/data/pure_symbolic_prove_summaries.json"
          - input: stderr
            output: "results/data/pure_symbolic_prove_summaries_errors.txt"
      # Run test-prove-dss
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_dss --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/pure_symbolic_prove_dss.txt"
          - input: stdout
            parser: pytest
            output: "results/data/pure_symbolic_prove_dss.json"
          - input: stderr
            output: "results/data/pure_symbolic_prove_dss_errors.txt"


  # Step 8: Summary Symbolic Execution performance testing
  - name: "summary_symbolic_performance"
    description: "Summary Symbolic Execution performance testing"
    enabled: true
    depends_on: ["build_kevm"]
    commands:
      # Cleanup: restore original files and remove backup files
      - command: "git checkout -- tests/specs && git clean -f -d tests/specs/"
        cwd: "repositories/evm-semantics"
        timeout: 30
        expected_exit_code: 0
      # Replace EDSL with EDSL-SUMMARY
      - command: "find repositories/evm-semantics/tests/specs -type f -exec sed -i.bak 's/EDSL/EDSL-SUMMARY/g' {} \\;"
        cwd: "."
        timeout: 30
        expected_exit_code: 0
      # Run test-prove-rules (booster mode)
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_rules --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/summary_symbolic_prove_rules_booster.txt"
          - input: stdout
            parser: pytest
            output: "results/data/summary_symbolic_prove_rules_booster.json"
          - input: stderr
            output: "results/data/summary_symbolic_prove_rules_booster_errors.txt"
      # Run test-prove-rules (booster-dev mode)
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_rules --tb=short --use-booster-dev --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/summary_symbolic_prove_rules_booster_dev.txt"
          - input: stdout
            parser: pytest
            output: "results/data/summary_symbolic_prove_rules_booster_dev.json"
          - input: stderr
            output: "results/data/summary_symbolic_prove_rules_booster_dev_errors.txt"
      # Run test-prove-summaries
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_summaries --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/summary_symbolic_prove_summaries.txt"
          - input: stdout
            parser: pytest
            output: "results/data/summary_symbolic_prove_summaries.json"
          - input: stderr
            output: "results/data/summary_symbolic_prove_summaries_errors.txt"
      # Run test-prove-dss
      - command: "uv run -- pytest src/tests/integration --verbose --durations=0 --dist=worksteal --maxfail=10 --numprocesses=4 -k test_prove_dss --tb=short --timeout=7200"
        cwd: "repositories/evm-semantics/kevm-pyk"
        timeout: 18000
        expected_exit_code: [0, 1, 2]
        continue_on_failure: true
        outputs:
          - input: stdout
            output: "results/data/summary_symbolic_prove_dss.txt"
          - input: stdout
            parser: pytest
            output: "results/data/summary_symbolic_prove_dss.json"
          - input: stderr
            output: "results/data/summary_symbolic_prove_dss_errors.txt"

  # Step 9: Data preprocessing
  - name: "data_preprocessing"
    description: "Preprocess evaluation data, generate standardized JSON format"
    commands:
      - command: "python eval-evm-summary/process_step3_data.py --input results/data/summarize_evaluation_results.json --output results/processed/step3_processed.json --prove results/data/prove_summaries_results.json"
        description: "Process step 3 data, generate category statistics"

  # Step 10: Visualization configuration
  - name: "visualization"
    description: "Generate charts and tables through declarative configuration"
    enabled: true
    depends_on: ["data_preprocessing"]
    
    # Visualization configuration - declarative definition of charts and tables to be generated
    visualization_config:
      data_directory: "results/processed"
      output_directory: "results/analysis"
      
      # LaTeX document generation configuration
      generate_latex_document: true
      document_title: "EVM Semantics Evaluation Results"
      document_author: "perfx"
      document_class: "article"
      
            # LaTeX table configuration
      tables:
        - name: "step3_opcode_list"
          title: "Opcode Summarization Results"
          input_file: "step3_processed.json"
          output_file: "tables/step3_opcode_list.tex"
          type: "opcode_list_table"
          data_path: "categories"
          use_tabularx: true  # Use tabularx because this table has long text that needs line breaks
          ignore_categories: ["Total"]  # Ignore Total category, not displayed in the first table
          columns:
            - field: "category"
              header: "Category"
              format: "text"
            - field: "success_opcodes"
              header: "Successful Opcodes"
              format: "text"
            - field: "failed_opcodes"
              header: "Failed Opcodes"
              format: "text"
        
        - name: "step3_category_statistics"
          title: "Statistics of Opcode Summarization"
          input_file: "step3_processed.json"
          output_file: "tables/step3_category_statistics.tex"
          type: "category_statistics_table"
          data_path: "categories"
          use_tabularx: false  # Don't use tabularx because this table is mainly numbers and doesn't need line breaks
          columns:
            - field: "category"
              header: "Category"
              format: "text"
            - field: "total_count"
              header: "#"
              format: "integer"
            - field: "success_rate"
              header: 'G. Succ.'
              format: "percentage"
            - field: "verification_success_rate"
              header: 'V. Succ.'
              format: "percentage"
            - field: "avg_time"
              header: "G. Time (s)"
              format: "float_1"
            - field: "avg_verification_time"
              header: "V. Time (s)"
              format: "float_1"
            - field: "avg_steps"
              header: "Steps"
              format: "float_1"
            - field: "avg_reduction"
              header: '$\Delta$ Steps'
              format: "percentage"
        

      
      # Chart configuration
      charts:

        - name: "concrete_performance_comparison"
          title: "Concrete Execution Performance Comparison"
          type: "performance_comparison"
          input_files:
            - "../data/pure_concrete_performance.json"
            - "../data/summary_concrete_performance.json"
          output_file: "charts/concrete_performance_comparison.pdf"
          # Data extraction configuration
          data_config:
            # Path to test results list
            test_results_path: "test_results"
            # Test ID field name
            test_id_field: "test_id"
            # Duration field name
            duration_field: "duration"
            # Status field name
            status_field: "status"
            # Error message field name (optional)
            error_field: "error_message"
          # Filter configuration
          filters:
            # Statuses to include (only process tests with these statuses)
            include_statuses: ["PASSED", "FAILED"]
            # Statuses to exclude (skip tests with these statuses)
            exclude_statuses: ["SKIPPED"]
            # Whether to only include successful tests
            only_successful: false
            # Whether to ignore tests with error messages
            ignore_errors: false
          # Chart configuration
          chart_config:
            # Label for first dataset
            dataset1_label: "Pure"
            # Label for second dataset
            dataset2_label: "Summary"
        
        # Performance distribution chart (speedup factor and performance improvement distribution)
        - name: "concrete_performance_distribution"
          title: "Concrete Execution Performance Distribution"
          type: "performance_distribution"
          input_files:
            - "../data/pure_concrete_performance.json"
            - "../data/summary_concrete_performance.json"
          output_file: "charts/concrete_performance_distribution.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
        
        # Performance scatter plot (Pure vs Summary execution time)
        - name: "concrete_performance_scatter"
          title: "Performance Comparison: Original vs Summarized Semantics"
          type: "performance_scatter"
          input_files:
            - "../data/pure_concrete_performance.json"
            - "../data/summary_concrete_performance.json"
          output_file: "charts/concrete_performance_scatter.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Original"
            dataset2_label: "Summarized"
        
        # Test case performance improvement chart (top 20 test cases with greatest improvement)
        - name: "concrete_test_case_improvement"
          title: "Top 20 Test Cases by Performance Improvement"
          type: "test_case_improvement"
          input_files:
            - "../data/pure_concrete_performance.json"
            - "../data/summary_concrete_performance.json"
          output_file: "charts/concrete_test_case_improvement.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
            top_n: 20
        
        # Symbolic performance scatter plot (booster_dev)
        - name: "symbolic_performance_scatter"
          title: "Symbolic Execution Performance Comparison: Original vs Summarized Semantics"
          type: "performance_scatter"
          input_files:
            - "../data/pure_symbolic_prove_rules_booster_dev.json"
            - "../data/summary_symbolic_prove_rules_booster_dev.json"
          output_file: "charts/symbolic_performance_scatter.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"

        # Symbolic test case improvement chart (booster_dev)
        - name: "symbolic_test_case_improvement"
          title: "Top Symbolic Test Cases by Performance Improvement"
          type: "test_case_improvement"
          input_files:
            - "../data/pure_symbolic_prove_rules_booster_dev.json"
            - "../data/summary_symbolic_prove_rules_booster_dev.json"
          output_file: "charts/symbolic_test_case_improvement.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
            top_n: 14

        # Symbolic performance scatter plot (booster)
        - name: "symbolic_booster_performance_scatter"
          title: "Symbolic Execution Performance Comparison (Booster): Original vs Summarized Semantics"
          type: "performance_scatter"
          input_files:
            - "../data/pure_symbolic_prove_rules_booster.json"
            - "../data/summary_symbolic_prove_rules_booster.json"
          output_file: "charts/symbolic_booster_performance_scatter.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"

        # Symbolic test case improvement chart (booster)
        - name: "symbolic_booster_test_case_improvement"
          title: "Top Symbolic Test Cases by Performance Improvement (Booster)"
          type: "test_case_improvement"
          input_files:
            - "../data/pure_symbolic_prove_rules_booster.json"
            - "../data/summary_symbolic_prove_rules_booster.json"
          output_file: "charts/symbolic_booster_test_case_improvement.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
            top_n: 20

        # Symbolic performance scatter plot (summaries)
        - name: "symbolic_summaries_performance_scatter"
          title: "Symbolic Execution Performance Comparison (Summaries): Original vs Summarized Semantics"
          type: "performance_scatter"
          input_files:
            - "../data/pure_symbolic_prove_summaries.json"
            - "../data/summary_symbolic_prove_summaries.json"
          output_file: "charts/symbolic_summaries_performance_scatter.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"

        # Symbolic test case improvement chart (summaries)
        - name: "symbolic_summaries_test_case_improvement"
          title: "Top Symbolic Test Cases by Performance Improvement (Summaries)"
          type: "test_case_improvement"
          input_files:
            - "../data/pure_symbolic_prove_summaries.json"
            - "../data/summary_symbolic_prove_summaries.json"
          output_file: "charts/symbolic_summaries_test_case_improvement.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
            top_n: 20

        # Symbolic performance scatter plot (dss)
        - name: "symbolic_dss_performance_scatter"
          title: "Symbolic Execution Performance Comparison (DSS): Original vs Summarized Semantics"
          type: "performance_scatter"
          input_files:
            - "../data/pure_symbolic_prove_dss.json"
            - "../data/summary_symbolic_prove_dss.json"
          output_file: "charts/symbolic_dss_performance_scatter.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"

        # Symbolic test case improvement chart (dss)
        - name: "symbolic_dss_test_case_improvement"
          title: "Top Symbolic Test Cases by Performance Improvement (DSS)"
          type: "test_case_improvement"
          input_files:
            - "../data/pure_symbolic_prove_dss.json"
            - "../data/summary_symbolic_prove_dss.json"
          output_file: "charts/symbolic_dss_test_case_improvement.pdf"
          data_config:
            test_results_path: "test_results"
            test_id_field: "test_id"
            duration_field: "duration"
            status_field: "status"
            error_field: "error_message"
          filters:
            include_statuses: ["PASSED", "FAILED"]
            exclude_statuses: ["SKIPPED"]
            only_successful: false
            ignore_errors: false
          chart_config:
            dataset1_label: "Pure"
            dataset2_label: "Summary"
            top_n: 20
        
 