"""
Test runner for EVM semantics with summary backend.

This module provides functionality to temporarily modify the interpreter.py file,
run conformance tests, and restore the original file.
"""

import sys
import shutil
import time
import subprocess
import re
from pathlib import Path
from typing import Dict, Tuple, Optional

from . import EVM_SEMANTICS_DIR, INTERPRETER_FILE, PROJECT_ROOT
from .utils import setup_compiler_for_macos, run_command, check_directories_and_files, print_project_info
from .evaluation_recorder import recorder

import os
import subprocess
import time
import re
import json
import shutil
from typing import Dict, List, Tuple, Any
from pathlib import Path


# å¤‡ä»½æ–‡ä»¶è·¯å¾„
BACKUP_FILE = INTERPRETER_FILE.with_suffix('.py.backup')


def backup_interpreter_file() -> bool:
    """
    å¤‡ä»½åŸå§‹çš„interpreter.pyæ–‡ä»¶
    """
    print(f"\n=== å¤‡ä»½interpreter.pyæ–‡ä»¶ ===")
    print(f"æºæ–‡ä»¶: {INTERPRETER_FILE}")
    print(f"å¤‡ä»½æ–‡ä»¶: {BACKUP_FILE}")
    
    if not INTERPRETER_FILE.exists():
        print(f"é”™è¯¯: interpreter.pyæ–‡ä»¶ä¸å­˜åœ¨: {INTERPRETER_FILE}")
        return False
    
    try:
        shutil.copy2(INTERPRETER_FILE, BACKUP_FILE)
        print("âœ“ interpreter.pyæ–‡ä»¶å¤‡ä»½å®Œæˆ")
        return True
    except Exception as e:
        print(f"âœ— å¤‡ä»½å¤±è´¥: {e}")
        return False


def modify_interpreter_file(semantics_mode: str = 'summary') -> bool:
    """
    ä¿®æ”¹interpreter.pyæ–‡ä»¶ï¼Œæ ¹æ®æŒ‡å®šçš„è¯­ä¹‰æ¨¡å¼æ›¿æ¢è¯­ä¹‰è°ƒç”¨
    
    Args:
        semantics_mode: è¯­ä¹‰æ¨¡å¼ï¼Œæ”¯æŒ 'pure', 'summary', 'default'
    """
    print(f"\n=== ä¿®æ”¹interpreter.pyæ–‡ä»¶ ===")
    
    # å®šä¹‰è¯­ä¹‰æ˜ å°„
    semantics_mapping = {
        'pure': "'evm-semantics.llvm-pure'",
        'summary': "'evm-semantics.llvm-summary'",
        'default': "'evm-semantics.llvm'"
    }
    
    if semantics_mode not in semantics_mapping:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„è¯­ä¹‰æ¨¡å¼: {semantics_mode}")
        return False
    
    original_text = "'evm-semantics.llvm'"
    new_text = semantics_mapping[semantics_mode]
    
    print(f"å°† {original_text} æ›¿æ¢ä¸º {new_text}")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(INTERPRETER_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ‰§è¡Œæ›¿æ¢
        if original_text not in content:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°è¦æ›¿æ¢çš„æ–‡æœ¬: {original_text}")
            return False
        
        modified_content = content.replace(original_text, new_text)
        
        # å†™å›æ–‡ä»¶
        with open(INTERPRETER_FILE, 'w', encoding='utf-8') as f:
            f.write(modified_content)
        
        print("âœ“ interpreter.pyæ–‡ä»¶ä¿®æ”¹å®Œæˆ")
        return True
    except Exception as e:
        print(f"âœ— ä¿®æ”¹å¤±è´¥: {e}")
        return False


def restore_interpreter_file() -> bool:
    """
    æ¢å¤åŸå§‹çš„interpreter.pyæ–‡ä»¶
    """
    print(f"\n=== æ¢å¤interpreter.pyæ–‡ä»¶ ===")
    print(f"ä»å¤‡ä»½æ–‡ä»¶æ¢å¤: {BACKUP_FILE}")
    
    if not BACKUP_FILE.exists():
        print(f"é”™è¯¯: å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {BACKUP_FILE}")
        return False
    
    try:
        shutil.copy2(BACKUP_FILE, INTERPRETER_FILE)
        print("âœ“ interpreter.pyæ–‡ä»¶æ¢å¤å®Œæˆ")
        return True
    except Exception as e:
        print(f"âœ— æ¢å¤å¤±è´¥: {e}")
        return False


def cleanup_backup_file() -> bool:
    """
    æ¸…ç†å¤‡ä»½æ–‡ä»¶
    """
    print(f"\n=== æ¸…ç†å¤‡ä»½æ–‡ä»¶ ===")
    print(f"åˆ é™¤å¤‡ä»½æ–‡ä»¶: {BACKUP_FILE}")
    
    if not BACKUP_FILE.exists():
        print("å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
        return True
    
    try:
        BACKUP_FILE.unlink()
        print("âœ“ å¤‡ä»½æ–‡ä»¶æ¸…ç†å®Œæˆ")
        return True
    except Exception as e:
        print(f"âœ— æ¸…ç†å¤±è´¥: {e}")
        return False





def run_conformance_tests_with_detailed_timing() -> Tuple[bool, Dict[str, float]]:
    """
    è¿è¡Œä¸€è‡´æ€§æµ‹è¯•å¹¶è·å–è¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹è€—æ—¶
    
    Returns:
        Tuple[bool, Dict[str, float]]: (æ˜¯å¦æˆåŠŸ, æµ‹è¯•ç”¨ä¾‹è€—æ—¶å­—å…¸)
    """
    print("\n=== æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯•ï¼ˆè¯¦ç»†è€—æ—¶ï¼‰ ===")
    print("æµ‹è¯•å‘½ä»¤: uv run -- pytest src/tests/integration/test_conformance.py --durations=0")
    print(f"å·¥ä½œç›®å½•: {EVM_SEMANTICS_DIR}/kevm-pyk")
    
    command = [
        "uv", "run", "--", "pytest",
        "src/tests/integration/test_conformance.py",
        "--durations=0",  # æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„è€—æ—¶
        "--verbose",
        "--no-header",    # ä¸æ˜¾ç¤ºpytestå¤´éƒ¨ä¿¡æ¯
        "--tb=no",        # ä¸æ˜¾ç¤ºtraceback
        "--timeout=3600"  # 1å°æ—¶è¶…æ—¶
    ]
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            cwd=EVM_SEMANTICS_DIR / "kevm-pyk"
        )
        
        duration = time.time() - start_time
        
        # è®°å½•å‘½ä»¤
        command_str = ' '.join(command)
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR / "kevm-pyk"),
            output=result.stdout,
            error=result.stderr,
            success=(result.returncode == 0),
            duration=duration
        )
        
        if result.returncode == 0:
            print("âœ“ ä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")
            # è§£æè¯¦ç»†è€—æ—¶
            test_durations = parse_pytest_durations(result.stdout)
            print(f"  è§£æåˆ° {len(test_durations)} ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è€—æ—¶ä¿¡æ¯")
            return True, test_durations
        else:
            print("âœ— ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥")
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
            return False, {}
            
    except Exception as e:
        duration = time.time() - start_time
        print(f"âœ— ä¸€è‡´æ€§æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}")
        
        # è®°å½•å¤±è´¥å‘½ä»¤
        command_str = ' '.join(command)
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR / "kevm-pyk"),
            output="",
            error=str(e),
            success=False,
            duration=duration
        )
        
        return False, {}


def analyze_pytest_output(output: str) -> Dict[str, Any]:
    """
    åˆ†æpytestè¾“å‡ºï¼Œæå–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        output: pytestçš„è¾“å‡ºå­—ç¬¦ä¸²
        
    Returns:
        Dict[str, Any]: åŒ…å«æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    passed = 0
    failed = 0
    skipped = 0
    error = 0
    
    # æŸ¥æ‰¾æµ‹è¯•ç»“æœæ‘˜è¦
    lines = output.split('\n')
    for line in lines:
        if 'passed' in line.lower() and 'failed' in line.lower():
            # å°è¯•è§£æç±»ä¼¼ "142 passed, 2 failed" çš„è¡Œ
            import re
            passed_match = re.search(r'(\d+)\s+passed', line)
            failed_match = re.search(r'(\d+)\s+failed', line)
            skipped_match = re.search(r'(\d+)\s+skipped', line)
            error_match = re.search(r'(\d+)\s+error', line)
            
            if passed_match:
                passed = int(passed_match.group(1))
            if failed_match:
                failed = int(failed_match.group(1))
            if skipped_match:
                skipped = int(skipped_match.group(1))
            if error_match:
                error = int(error_match.group(1))
            break
    
    total = passed + failed + skipped + error
    success_rate = (passed / total * 100) if total > 0 else 0
    
    return {
        'passed': passed,
        'failed': failed,
        'skipped': skipped,
        'error': error,
        'total': total,
        'success_rate': success_rate
    }


def parse_pytest_durations(output: str) -> Dict[str, float]:
    """
    è§£æpytest --durations=0çš„è¾“å‡ºï¼Œæå–æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è€—æ—¶
    
    Args:
        output: pytestçš„è¾“å‡ºå†…å®¹
        
    Returns:
        Dict[str, float]: æµ‹è¯•ç”¨ä¾‹åç§°åˆ°è€—æ—¶çš„æ˜ å°„
    """
    durations = {}
    
    # pytest --durations=0çš„è¾“å‡ºæ ¼å¼å¯èƒ½åŒ…æ‹¬ï¼š
    # 1. test_conformanceæ ¼å¼ï¼š
    # 171.42s call     src/tests/integration/test_conformance.py::test_bchain[Pyspecs/cancun/eip4844_blobs/external_vectors.json]
    # 2. proveæµ‹è¯•æ ¼å¼ï¼š
    # 45.23s call     src/tests/integration/test_prove.py::test_prove_rules
    # 32.15s call     src/tests/integration/test_prove.py::test_prove_summaries
    # 28.67s call     src/tests/integration/test_prove.py::test_prove_dss
    
    # ä½¿ç”¨æ›´é€šç”¨çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è€—æ—¶ä¿¡æ¯
    pattern = r'(\d+\.?\d*)s\s+call\s+(src/tests/integration/[^\s]+)'
    
    for line in output.split('\n'):
        match = re.search(pattern, line)
        if match:
            duration = float(match.group(1))
            test_name = match.group(2)
            durations[test_name] = duration
    
    return durations


def run_prove_summaries_test() -> bool:
    """
    åœ¨evm-semanticsç›®å½•ä¸‹æ‰§è¡Œprove summariesæµ‹è¯•
    
    Returns:
        bool: æµ‹è¯•æ‰§è¡Œæ˜¯å¦æˆåŠŸ
    """
    print("\n=== æ‰§è¡Œprove summariesæµ‹è¯• ===")
    print("æµ‹è¯•å‘½ä»¤: uv run -- pytest src/tests/integration/test_prove.py::test_prove_summaries")
    print(f"å·¥ä½œç›®å½•: {EVM_SEMANTICS_DIR}/kevm-pyk")
    print("è¶…æ—¶æ—¶é—´: 2å°æ—¶")
    
    try:
        result = subprocess.run(
            [
                "uv", "run", "--", "pytest",
                "src/tests/integration/test_prove.py::test_prove_summaries",
                "--verbose", "--durations=0", "--dist=worksteal", "--numprocesses=8", "--timeout=7200"
            ],
            capture_output=True,
            text=True,
            cwd=EVM_SEMANTICS_DIR / "kevm-pyk"  # Execute in kevm-pyk directory
        )
        
        if result.returncode == 0:
            print("âœ“ prove summariesæµ‹è¯•å®Œæˆ")
            print("æµ‹è¯•è¾“å‡º:")
            print(result.stdout)
            return True
        else:
            print("âœ— prove summariesæµ‹è¯•å¤±è´¥")
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
            print("æ ‡å‡†è¾“å‡º:")
            print(result.stdout)
            return False
            
    except subprocess.CalledProcessError as e:
        print(f"âœ— prove summariesæµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âœ— prove summariesæµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
        return False





def run_conformance_test_with_semantics(semantics_mode: str, record_results: bool = False) -> Tuple[bool, Dict[str, float]]:
    """
    ä½¿ç”¨æŒ‡å®šè¯­ä¹‰è¿è¡Œä¸€è‡´æ€§æµ‹è¯•å¹¶è·å–è¯¦ç»†è€—æ—¶
    
    Args:
        semantics_mode: è¯­ä¹‰æ¨¡å¼ï¼Œæ”¯æŒ 'pure', 'summary', 'default'
        record_results: æ˜¯å¦è®°å½•ç»“æœåˆ°recorderä¸­
    
    Returns:
        Tuple[bool, Dict[str, float]]: (æ˜¯å¦æˆåŠŸ, æµ‹è¯•ç”¨ä¾‹è€—æ—¶å­—å…¸)
    """
    semantics_names = {
        'pure': 'åŸå§‹è¯­ä¹‰ (llvm-pure)',
        'summary': 'summarizedè¯­ä¹‰ (llvm-summary)',
        'default': 'é»˜è®¤è¯­ä¹‰ (llvm)'
    }
    
    test_name = f"conformance_test_{semantics_mode}"
    
    if record_results:
        recorder.start_test(test_name)
    
    print(f"\n=== ä½¿ç”¨{semantics_names.get(semantics_mode, semantics_mode)}è¿è¡Œä¸€è‡´æ€§æµ‹è¯• ===")
    
    # æ£€æŸ¥ç›®å½•å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not check_directories_and_files(EVM_SEMANTICS_DIR, INTERPRETER_FILE, description="è·¯å¾„"):
        if record_results:
            recorder.end_test(test_name, False, "è·¯å¾„æ£€æŸ¥å¤±è´¥")
        return False, {}
    
    # ä¸ºmacOSè®¾ç½®ç¼–è¯‘å™¨
    setup_compiler_for_macos()
    
    success = False
    test_durations = {}
    
    try:
        # 1. å¤‡ä»½åŸå§‹æ–‡ä»¶
        if not backup_interpreter_file():
            if record_results:
                recorder.end_test(test_name, False, "å¤‡ä»½æ–‡ä»¶å¤±è´¥")
            return False, {}
        
        # 2. ä¿®æ”¹æ–‡ä»¶ä½¿ç”¨æŒ‡å®šè¯­ä¹‰
        if not modify_interpreter_file(semantics_mode):
            if record_results:
                recorder.end_test(test_name, False, "ä¿®æ”¹æ–‡ä»¶å¤±è´¥")
            return False, {}
        
        # 3. æ‰§è¡Œæµ‹è¯•å¹¶è·å–è¯¦ç»†è€—æ—¶
        success, test_durations = run_conformance_tests_with_detailed_timing()
        
    except Exception as e:
        print(f"\n=== æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ ===")
        print(f"é”™è¯¯: {e}")
        success = False
        if record_results:
            recorder.end_test(test_name, False, str(e))
    
    finally:
        # 4. æ¢å¤åŸå§‹æ–‡ä»¶
        if not restore_interpreter_file():
            print("è­¦å‘Š: æ–‡ä»¶æ¢å¤å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥")
        
        # 5. æ¸…ç†å¤‡ä»½æ–‡ä»¶
        cleanup_backup_file()
    
    if record_results and success:
        recorder.end_test(test_name, True)
    
    return success, test_durations


def run_concrete_execution_performance_comparison() -> bool:
    """
    æ‰§è¡Œconcrete executionæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    Returns:
        bool: æ€§èƒ½å¯¹æ¯”æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    print("\n=== å¼€å§‹Concrete Executionæ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    recorder.start_test("concrete_execution_performance")
    
    try:
        # 1. ä½¿ç”¨åŸå§‹è¯­ä¹‰è¿è¡Œæµ‹è¯•
        print("\n" + "="*50)
        print("PHASE 1: ä½¿ç”¨åŸå§‹è¯­ä¹‰ (llvm-pure)")
        print("="*50)
        pure_success, pure_durations = run_conformance_test_with_semantics('pure', record_results=False)
        
        if not pure_success:
            print("âœ— åŸå§‹è¯­ä¹‰æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
            recorder.end_test("concrete_execution_performance", False, "åŸå§‹è¯­ä¹‰æµ‹è¯•å¤±è´¥")
            return False
        
        # 2. ä½¿ç”¨summarizedè¯­ä¹‰è¿è¡Œæµ‹è¯•
        print("\n" + "="*50)
        print("PHASE 2: ä½¿ç”¨summarizedè¯­ä¹‰ (llvm-summary)")
        print("="*50)
        summary_success, summary_durations = run_conformance_test_with_semantics('summary', record_results=False)
        
        if not summary_success:
            print("âœ— summarizedè¯­ä¹‰æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
            recorder.end_test("concrete_execution_performance", False, "summarizedè¯­ä¹‰æµ‹è¯•å¤±è´¥")
            return False
        
        # 3. åˆ†ææ€§èƒ½å¯¹æ¯”ç»“æœ
        print("\n" + "="*50)
        print("PHASE 3: æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print("="*50)
        
        # å°†ç»“æœå­˜å‚¨åˆ°recorderä¸­
        recorder.results["concrete_execution_performance"] = {
            "pure_semantics": {
                "success": pure_success,
                "test_durations": pure_durations
            },
            "summary_semantics": {
                "success": summary_success,
                "test_durations": summary_durations
            }
        }
        
        # æ‰§è¡Œæ€§èƒ½åˆ†æ
        performance_analysis = recorder.analyze_concrete_execution_performance(pure_durations, summary_durations)
        recorder.results["concrete_execution_performance"]["analysis"] = performance_analysis
        
        # æ‰“å°æ€§èƒ½åˆ†æç»“æœ
        print(f"åŸå§‹è¯­ä¹‰æ€»è€—æ—¶: {performance_analysis['pure_total_time']:.2f}s")
        print(f"Summarizedè¯­ä¹‰æ€»è€—æ—¶: {performance_analysis['summary_total_time']:.2f}s")
        print(f"æ€»ä½“æ€§èƒ½æå‡: {performance_analysis['total_speedup']:.2f}x")
        print(f"æ€»ä½“æ”¹è¿›ç™¾åˆ†æ¯”: {performance_analysis['total_improvement']:.1f}%")
        
        # è®°å½•æˆåŠŸ
        recorder.end_test("concrete_execution_performance", True)
        
        print("\nâœ“ Concrete Executionæ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        error_msg = f"æ€§èƒ½å¯¹æ¯”æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}"
        print(f"âœ— {error_msg}")
        recorder.end_test("concrete_execution_performance", False, error_msg)
        return False


def run_conformance_test_with_summary() -> bool:
    """è¿è¡Œä¸€è‡´æ€§æµ‹è¯•å¹¶è®°å½•ç»“æœï¼ˆä½¿ç”¨summarizedè¯­ä¹‰ï¼‰"""
    success, _ = run_conformance_test_with_semantics('summary', record_results=True)
    return success


def run_prove_summaries_test_with_recording() -> bool:
    """è¿è¡Œprove summariesæµ‹è¯•å¹¶è®°å½•ç»“æœ"""
    recorder.start_test("prove_summaries_test")
    try:
        # è¿™é‡Œéœ€è¦é‡æ–°å®ç°ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ•è·è¾“å‡º
        print("\n=== æ‰§è¡Œprove summariesæµ‹è¯• ===")
        print("æµ‹è¯•å‘½ä»¤: uv run -- pytest src/tests/integration/test_prove.py::test_prove_summaries")
        print(f"å·¥ä½œç›®å½•: {EVM_SEMANTICS_DIR}/kevm-pyk")
        print("è¶…æ—¶æ—¶é—´: 2å°æ—¶")
        
        start_time = time.time()
        command = [
            "uv", "run", "--", "pytest",
            "src/tests/integration/test_prove.py::test_prove_summaries",
            "--verbose", "--durations=0", "--dist=worksteal", "--numprocesses=8", "--timeout=7200"
        ]
        command_str = ' '.join(command)
        
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            cwd=EVM_SEMANTICS_DIR / "kevm-pyk"  # Execute in kevm-pyk directory
        )
        
        duration = time.time() - start_time
        
        # è®°å½•å‘½ä»¤
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR / "kevm-pyk"),
            output=result.stdout,
            error=result.stderr,
            success=(result.returncode == 0),
            duration=duration
        )
        
        if result.returncode == 0:
            print("âœ“ prove summariesæµ‹è¯•å®Œæˆ")
            print("æµ‹è¯•è¾“å‡º:")
            print(result.stdout)
            
            # è®°å½•ç»“æœ
            recorder.end_test("prove_summaries_test", True)
            recorder.update_test_output("prove_summaries_test", result.stdout, result.stderr)
            
            return True
        else:
            print("âœ— prove summariesæµ‹è¯•å¤±è´¥")
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
            print("æ ‡å‡†è¾“å‡º:")
            print(result.stdout)
            
            # è®°å½•ç»“æœ
            recorder.end_test("prove_summaries_test", False)
            recorder.update_test_output("prove_summaries_test", result.stdout, result.stderr)
            
            return False
            
    except subprocess.CalledProcessError as e:
        duration = time.time() - start_time
        print(f"âœ— prove summariesæµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}")
        
        # è®°å½•å¤±è´¥å‘½ä»¤
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR / "kevm-pyk"),
            output=e.stdout,
            error=e.stderr,
            success=False,
            duration=duration
        )
        
        recorder.end_test("prove_summaries_test", False, str(e))
        return False
    except Exception as e:
        duration = time.time() - start_time
        print(f"âœ— prove summariesæµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
        
        # è®°å½•å¼‚å¸¸å‘½ä»¤
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR / "kevm-pyk"),
            output="",
            error=str(e),
            success=False,
            duration=duration
        )
        
        recorder.end_test("prove_summaries_test", False, str(e))
        return False


def backup_and_restore_specs() -> bool:
    """
    ä½¿ç”¨git checkoutæ¢å¤evm-semantics/tests/specsæ–‡ä»¶å¤¹åˆ°åŸå§‹çŠ¶æ€
    
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    specs_dir = EVM_SEMANTICS_DIR / "tests" / "specs"
    if not specs_dir.exists():
        print(f"âœ— specsç›®å½•ä¸å­˜åœ¨: {specs_dir}")
        return False
    
    print("ğŸ”„ ä½¿ç”¨git checkoutæ¢å¤åŸå§‹æ–‡ä»¶...")
    try:
        # ä½¿ç”¨git checkoutæ¢å¤specsç›®å½•åˆ°åŸå§‹çŠ¶æ€
        result = subprocess.run(
            ['git', 'checkout', '--', str(specs_dir)],
            capture_output=True,
            text=True,
            cwd=EVM_SEMANTICS_DIR
        )
        
        if result.returncode == 0:
            print("âœ“ å·²ä½¿ç”¨git checkoutæ¢å¤åŸå§‹æ–‡ä»¶")
            return True
        else:
            print(f"âœ— git checkoutå¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âœ— æ¢å¤æ–‡ä»¶å¤±è´¥: {e}")
        return False


def replace_edsl_in_specs(target_module: str) -> bool:
    """
    åœ¨evm-semantics/tests/specsæ–‡ä»¶å¤¹ä¸­æ›¿æ¢æ‰€æœ‰æ–‡ä»¶ä¸­çš„EDSLå¼•ç”¨
    
    Args:
        target_module: ç›®æ ‡æ¨¡å—åï¼Œå¦‚ 'EDSL-PURE' æˆ– 'EDSL-SUMMARY'
        
    Returns:
        bool: æ›¿æ¢æ˜¯å¦æˆåŠŸ
    """
    specs_dir = EVM_SEMANTICS_DIR / "tests" / "specs"
    if not specs_dir.exists():
        print(f"âœ— specsç›®å½•ä¸å­˜åœ¨: {specs_dir}")
        return False
    
    print(f"ğŸ”„ æ›¿æ¢EDSLä¸º{target_module}...")
    replaced_files = 0
    
    # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œä¸ä»…ä»…æ˜¯.kæ–‡ä»¶
    for file_path in specs_dir.rglob("*"):
        if file_path.is_file():
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ›¿æ¢imports EDSL
                original_content = content
                content = content.replace('EDSL', target_module)
                
                # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å›æ–‡ä»¶
                if content != original_content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    replaced_files += 1
                    print(f"  âœ“ æ›¿æ¢: {file_path.relative_to(specs_dir)}")
                    
            except Exception as e:
                print(f"  âœ— å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                return False
    
    print(f"âœ“ æˆåŠŸæ›¿æ¢äº† {replaced_files} ä¸ªæ–‡ä»¶ä¸­çš„EDSLå¼•ç”¨")
    return True


def run_symbolic_prove_test_with_semantics(semantics_mode: str, record_results: bool = False) -> Tuple[bool, Dict[str, Dict[str, float]]]:
    """
    ä½¿ç”¨æŒ‡å®šè¯­ä¹‰è¿è¡Œsymbolic execution proveæµ‹è¯•å¹¶è·å–è¯¦ç»†æµ‹è¯•ç”¨ä¾‹è€—æ—¶
    
    Args:
        semantics_mode: è¯­ä¹‰æ¨¡å¼ï¼Œæ”¯æŒ 'pure', 'summary'
        record_results: æ˜¯å¦è®°å½•ç»“æœåˆ°recorderä¸­
        
    Returns:
        Tuple[bool, Dict[str, Dict[str, float]]]: (æ˜¯å¦æˆåŠŸ, æµ‹è¯•å¥—ä»¶åˆ°æµ‹è¯•ç”¨ä¾‹è€—æ—¶çš„æ˜ å°„)
    """
    semantics_names = {
        'pure': 'æ— ä¼˜åŒ–è¯­ä¹‰ (EDSL-PURE)',
        'summary': 'summarizedè¯­ä¹‰ (EDSL-SUMMARY)',
    }
    
    test_name = f"symbolic_prove_test_{semantics_mode}"
    
    if record_results:
        recorder.start_test(test_name)
    
    print(f"\n=== ä½¿ç”¨{semantics_names.get(semantics_mode, semantics_mode)}è¿è¡Œsymbolic proveæµ‹è¯• ===")
    
    # 1. æ›¿æ¢ä¸ºæŒ‡å®šè¯­ä¹‰
    print(f"\n1. æ›¿æ¢ä¸º{semantics_mode}è¯­ä¹‰...")
    if semantics_mode == 'pure':
        target_module = 'EDSL-PURE'
    elif semantics_mode == 'summary':
        target_module = 'EDSL-SUMMARY'
    else:
        print(f"âœ— ä¸æ”¯æŒçš„è¯­ä¹‰æ¨¡å¼: {semantics_mode}")
        if record_results:
            recorder.end_test(test_name, False, f"ä¸æ”¯æŒçš„è¯­ä¹‰æ¨¡å¼: {semantics_mode}")
        return False, {}
    
    if not replace_edsl_in_specs(target_module):
        print("âœ— è¯­ä¹‰æ›¿æ¢å¤±è´¥")
        if record_results:
            recorder.end_test(test_name, False, "è¯­ä¹‰æ›¿æ¢å¤±è´¥")
        return False, {}
    
    # 2. è¿è¡ŒæŒ‡å®šçš„æµ‹è¯•å¥—ä»¶
    print("\n2. è¿è¡Œæµ‹è¯•å¥—ä»¶...")
    test_configs = [
        # test-prove-rules éœ€è¦è¿è¡Œ booster å’Œ booster-dev ä¸¤ç§æ¨¡å¼
        {'suite': 'test-prove-rules', 'booster_mode': 'booster', 'timeout': 120},
        {'suite': 'test-prove-rules', 'booster_mode': 'booster-dev', 'timeout': 120},
        # test-prove-summaries å’Œ test-prove-dss ä½¿ç”¨é»˜è®¤é…ç½®
        {'suite': 'test-prove-summaries', 'booster_mode': 'booster', 'timeout': 120},
        {'suite': 'test-prove-dss', 'booster_mode': 'booster', 'timeout': 120},
    ]
    
    suite_test_durations = {}
    successful_suites = 0
    total_suites = len(test_configs)
    
    for config in test_configs:
        suite_name = config['suite']
        booster_mode = config['booster_mode']
        timeout = config['timeout']
        
        # ä¸ºtest-prove-rulesåˆ›å»ºä¸åŒçš„å¥—ä»¶åç§°
        if suite_name == 'test-prove-rules':
            suite_key = f"{suite_name}-{booster_mode}"
        else:
            suite_key = suite_name
        
        print(f"\n--- è¿è¡Œ {suite_key} (è¶…æ—¶: {timeout}åˆ†é’Ÿ) ---")
        
        success, test_durations = run_single_prove_test_suite_with_booster(
            suite_name, booster_mode, timeout
        )
        
        suite_test_durations[suite_key] = test_durations
        
        if success:
            successful_suites += 1
            total_duration = sum(test_durations.values()) if test_durations else 0
            test_count = len(test_durations) if test_durations else 0
            print(f"âœ“ {suite_key} æµ‹è¯•æˆåŠŸ (æ€»è€—æ—¶: {total_duration:.2f}ç§’, æµ‹è¯•ç”¨ä¾‹æ•°: {test_count})")
        else:
            total_duration = sum(test_durations.values()) if test_durations else 0
            test_count = len(test_durations) if test_durations else 0
            print(f"âš ï¸  {suite_key} æµ‹è¯•éƒ¨åˆ†å¤±è´¥ (æ€»è€—æ—¶: {total_duration:.2f}ç§’, æµ‹è¯•ç”¨ä¾‹æ•°: {test_count})")
    
    # 3. æ¢å¤åŸå§‹æ–‡ä»¶
    print("\n3. æ¢å¤åŸå§‹æ–‡ä»¶...")
    if not backup_and_restore_specs():
        print("âš ï¸  è­¦å‘Š: æ¢å¤åŸå§‹æ–‡ä»¶å¤±è´¥")
    
    # è®¡ç®—æ•´ä½“æˆåŠŸç‡
    success_rate = (successful_suites / total_suites * 100) if total_suites > 0 else 0
    overall_success = success_rate >= 50.0  # åªè¦50%ä»¥ä¸Šçš„å¥—ä»¶æˆåŠŸå°±è®¤ä¸ºæ˜¯æˆåŠŸçš„
    
    print(f"\næµ‹è¯•å¥—ä»¶ç»Ÿè®¡: {successful_suites}/{total_suites} æˆåŠŸ, æˆåŠŸç‡: {success_rate:.1f}%")
    
    # è®°å½•ç»“æœ
    if record_results:
        total_tests = sum(len(durations) for durations in suite_test_durations.values())
        recorder.end_test(test_name, overall_success, f"æµ‹è¯•å¥—ä»¶æ•°é‡: {len(suite_test_durations)}, æˆåŠŸå¥—ä»¶: {successful_suites}/{total_suites}, æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {total_tests}")
    
    return overall_success, suite_test_durations


def run_single_prove_test_suite_with_booster(suite_name: str, booster_mode: str, timeout_minutes: int) -> Tuple[bool, Dict[str, float]]:
    """
    è¿è¡Œå•ä¸ªproveæµ‹è¯•å¥—ä»¶ï¼Œæ”¯æŒæŒ‡å®šboosteræ¨¡å¼
    
    Args:
        suite_name: æµ‹è¯•å¥—ä»¶åç§° (å¦‚ 'test-prove-rules')
        booster_mode: boosteræ¨¡å¼ ('booster' æˆ– 'booster-dev')
        timeout_minutes: è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        
    Returns:
        Tuple[bool, Dict[str, float]]: (æ˜¯å¦æˆåŠŸ, æµ‹è¯•ç”¨ä¾‹è€—æ—¶å­—å…¸)
    """
    start_time = time.time()
    
    try:
        # æ ¹æ®boosteræ¨¡å¼æ„å»ºä¸åŒçš„pytestå‚æ•°ï¼Œå¹¶æ·»åŠ timeoutè®¾ç½®
        timeout_seconds = timeout_minutes * 60
        if booster_mode == 'booster-dev':
            pytest_args = f'-v --tb=short --durations=0 --use-booster-dev --timeout={timeout_seconds}'
        else:
            pytest_args = f'-v --tb=short --durations=0 --timeout={timeout_seconds}'
        
        # æ„å»ºmakeå‘½ä»¤
        command = ['make', suite_name, f'PYTEST_ARGS={pytest_args}']
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
        print(f"Boosteræ¨¡å¼: {booster_mode}")
        print(f"è¶…æ—¶æ—¶é—´: {timeout_minutes}åˆ†é’Ÿ ({timeout_seconds}ç§’)")
        
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            cwd=EVM_SEMANTICS_DIR
        )
        
        overall_duration = time.time() - start_time
        
        # è®°å½•å‘½ä»¤æ‰§è¡Œ
        command_str = ' '.join(command)
        recorder.add_command(
            command=command_str,
            cwd=str(EVM_SEMANTICS_DIR),
            output=result.stdout,
            error=result.stderr,
            success=(result.returncode == 0),
            duration=overall_duration
        )
        
        # è§£æpytestè¾“å‡ºè·å–æ¯ä¸ªæµ‹è¯•çš„è€—æ—¶
        test_durations = parse_pytest_durations(result.stdout)
        
        # åˆ†ææµ‹è¯•ç»“æœ
        test_stats = analyze_pytest_output(result.stdout)
        
        # å¯¹äºsymbolic executionæµ‹è¯•ï¼Œå³ä½¿æœ‰å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œåªè¦å¤§éƒ¨åˆ†æµ‹è¯•æˆåŠŸå°±è®¤ä¸ºæ˜¯æˆåŠŸçš„
        # è¿™æ ·å¯ä»¥æ”¶é›†åˆ°æ›´å¤šçš„æ€§èƒ½æ•°æ®
        success = (result.returncode == 0) or (test_stats['success_rate'] > 50.0)
        
        if not success:
            print(f"æµ‹è¯•å¥—ä»¶è¾“å‡º (æœ€å500è¡Œ):")
            print('\n'.join(result.stdout.split('\n')[-500:]))
            print(f"é”™è¯¯è¾“å‡º:")
            print(result.stderr[-1000:])
        
        print(f"æµ‹è¯•ç»Ÿè®¡: {test_stats['passed']} é€šè¿‡, {test_stats['failed']} å¤±è´¥, æˆåŠŸç‡: {test_stats['success_rate']:.1f}%")
        
        # å¯¹äºsymbolic executionæµ‹è¯•ï¼Œæˆ‘ä»¬è¿”å›è¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹è€—æ—¶
        # è¿™æ ·å¯ä»¥è¿›è¡Œç»†ç²’åº¦çš„æ€§èƒ½åˆ†æ
        return success, test_durations
        
    except Exception as e:
        overall_duration = time.time() - start_time
        print(f"âœ— è¿è¡Œæµ‹è¯•å¥—ä»¶ {suite_name} ({booster_mode}) æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return False, {}


def calculate_test_statistics(suite_durations: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
    """
    è®¡ç®—æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        suite_durations: æµ‹è¯•å¥—ä»¶åˆ°æµ‹è¯•ç”¨ä¾‹è€—æ—¶çš„æ˜ å°„
        
    Returns:
        Dict[str, Any]: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    total_suites = len(suite_durations)
    successful_suites = 0
    total_test_cases = 0
    successful_test_cases = 0
    
    for suite_name, test_durations in suite_durations.items():
        suite_test_count = len(test_durations)
        total_test_cases += suite_test_count
        
        # å‡è®¾å¦‚æœå¥—ä»¶æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œå°±è®¤ä¸ºæ˜¯æˆåŠŸçš„
        if suite_test_count > 0:
            successful_suites += 1
            successful_test_cases += suite_test_count
    
    success_rate = (successful_suites / total_suites * 100) if total_suites > 0 else 0
    
    return {
        'total_suites': total_suites,
        'successful_suites': successful_suites,
        'success_rate': success_rate,
        'total_test_cases': total_test_cases,
        'successful_test_cases': successful_test_cases
    }


def run_symbolic_execution_performance_comparison() -> bool:
    """
    æ‰§è¡Œsymbolic executionæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    Returns:
        bool: æ€§èƒ½å¯¹æ¯”æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    print("\n=== å¼€å§‹Symbolic Executionæ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    recorder.start_test("symbolic_execution_performance")
    
    try:
        # 1. ä½¿ç”¨åŸå§‹è¯­ä¹‰è¿è¡Œæµ‹è¯•
        print("\n" + "="*50)
        print("PHASE 1: ä½¿ç”¨åŸå§‹è¯­ä¹‰ (evm-semantics.haskell)")
        print("="*50)
        haskell_success, haskell_durations = run_symbolic_prove_test_with_semantics('pure', record_results=False)
        
        # 2. ä½¿ç”¨summarizedè¯­ä¹‰è¿è¡Œæµ‹è¯•
        print("\n" + "="*50)
        print("PHASE 2: ä½¿ç”¨summarizedè¯­ä¹‰ (evm-semantics.haskell-summary)")
        print("="*50)
        haskell_summary_success, haskell_summary_durations = run_symbolic_prove_test_with_semantics('summary', record_results=False)
        
        # 3. åˆ†ææ€§èƒ½å¯¹æ¯”ç»“æœ
        print("\n" + "="*50)
        print("PHASE 3: æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print("="*50)
        
        # è®¡ç®—æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        haskell_stats = calculate_test_statistics(haskell_durations)
        haskell_summary_stats = calculate_test_statistics(haskell_summary_durations)
        
        print(f"åŸå§‹è¯­ä¹‰æµ‹è¯•ç»Ÿè®¡:")
        print(f"  - æˆåŠŸå¥—ä»¶: {haskell_stats['successful_suites']}/{haskell_stats['total_suites']}")
        print(f"  - æˆåŠŸç‡: {haskell_stats['success_rate']:.1f}%")
        print(f"  - æ€»æµ‹è¯•ç”¨ä¾‹: {haskell_stats['total_test_cases']}")
        print(f"  - æˆåŠŸæµ‹è¯•ç”¨ä¾‹: {haskell_stats['successful_test_cases']}")
        
        print(f"Summarizedè¯­ä¹‰æµ‹è¯•ç»Ÿè®¡:")
        print(f"  - æˆåŠŸå¥—ä»¶: {haskell_summary_stats['successful_suites']}/{haskell_summary_stats['total_suites']}")
        print(f"  - æˆåŠŸç‡: {haskell_summary_stats['success_rate']:.1f}%")
        print(f"  - æ€»æµ‹è¯•ç”¨ä¾‹: {haskell_summary_stats['total_test_cases']}")
        print(f"  - æˆåŠŸæµ‹è¯•ç”¨ä¾‹: {haskell_summary_stats['successful_test_cases']}")
        
        # å°†ç»“æœå­˜å‚¨åˆ°recorderä¸­
        recorder.results["symbolic_execution_performance"] = {
            "haskell_semantics": {
                "success": haskell_success,
                "suite_durations": haskell_durations,
                "statistics": haskell_stats
            },
            "haskell_summary_semantics": {
                "success": haskell_summary_success,
                "suite_durations": haskell_summary_durations,
                "statistics": haskell_summary_stats
            }
        }
        
        # æ‰§è¡Œæ€§èƒ½åˆ†æï¼ˆåªå¯¹æˆåŠŸçš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œæ¯”è¾ƒï¼‰
        if haskell_stats['successful_test_cases'] > 0 and haskell_summary_stats['successful_test_cases'] > 0:
            performance_analysis = recorder.analyze_symbolic_execution_performance(haskell_durations, haskell_summary_durations)
            recorder.results["symbolic_execution_performance"]["analysis"] = performance_analysis
            
            # æ‰“å°æ€§èƒ½åˆ†æç»“æœ
            print(f"\næ€§èƒ½å¯¹æ¯”åˆ†æ:")
            print(f"åŸå§‹è¯­ä¹‰æ€»è€—æ—¶: {performance_analysis['haskell_total_time']:.2f}s")
            print(f"Summarizedè¯­ä¹‰æ€»è€—æ—¶: {performance_analysis['haskell_summary_total_time']:.2f}s")
            print(f"æ€»ä½“æ€§èƒ½æå‡: {performance_analysis['total_speedup']:.2f}x")
            print(f"æ€»ä½“æ”¹è¿›ç™¾åˆ†æ¯”: {performance_analysis['total_improvement']:.1f}%")
            print(f"å¯æ¯”è¾ƒçš„å¥—ä»¶æ•°é‡: {performance_analysis['statistics']['num_comparable_suites']}/{performance_analysis['statistics']['num_suites']}")
            print(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {performance_analysis['statistics']['num_test_cases']}")
            print(f"å¯æ¯”è¾ƒçš„æµ‹è¯•ç”¨ä¾‹æ•°: {performance_analysis['statistics']['num_comparable_test_cases']}")
            
            if performance_analysis['statistics']['num_comparable_suites'] > 0:
                print(f"å¥—ä»¶å¹³å‡æ€§èƒ½æå‡: {performance_analysis['statistics']['avg_suite_speedup']:.2f}x")
                print(f"å¥—ä»¶æœ€å¤§æ€§èƒ½æå‡: {performance_analysis['statistics']['max_suite_speedup']:.2f}x")
                print(f"å¥—ä»¶æœ€å°æ€§èƒ½æå‡: {performance_analysis['statistics']['min_suite_speedup']:.2f}x")
            
            if performance_analysis['statistics']['num_comparable_test_cases'] > 0:
                print(f"æµ‹è¯•ç”¨ä¾‹å¹³å‡æ€§èƒ½æå‡: {performance_analysis['statistics']['avg_test_speedup']:.2f}x")
                print(f"æµ‹è¯•ç”¨ä¾‹æœ€å¤§æ€§èƒ½æå‡: {performance_analysis['statistics']['max_test_speedup']:.2f}x")
                print(f"æµ‹è¯•ç”¨ä¾‹æœ€å°æ€§èƒ½æå‡: {performance_analysis['statistics']['min_test_speedup']:.2f}x")
        else:
            print(f"\nâš ï¸  æ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”åˆ†æï¼š")
            if haskell_stats['successful_test_cases'] == 0:
                print(f"  - åŸå§‹è¯­ä¹‰æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç”¨ä¾‹")
            if haskell_summary_stats['successful_test_cases'] == 0:
                print(f"  - Summarizedè¯­ä¹‰æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç”¨ä¾‹")
        
        # åˆ¤æ–­æ•´ä½“æˆåŠŸï¼šåªè¦ä¸¤ä¸ªè¯­ä¹‰éƒ½æœ‰ä¸€å®šæ•°é‡çš„æˆåŠŸæµ‹è¯•ç”¨ä¾‹å°±è®¤ä¸ºæ˜¯æˆåŠŸçš„
        overall_success = (haskell_stats['successful_test_cases'] > 0 and haskell_summary_stats['successful_test_cases'] > 0)
        
        if overall_success:
            print(f"\nâœ“ Symbolic Executionæ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ")
            recorder.end_test("symbolic_execution_performance", True)
        else:
            print(f"\nâš ï¸  Symbolic Executionæ€§èƒ½å¯¹æ¯”æµ‹è¯•éƒ¨åˆ†å®Œæˆï¼ˆæµ‹è¯•ç”¨ä¾‹æˆåŠŸç‡è¾ƒä½ï¼‰")
            recorder.end_test("symbolic_execution_performance", False, "æµ‹è¯•ç”¨ä¾‹æˆåŠŸç‡è¾ƒä½")
        
        return overall_success
        
    except Exception as e:
        error_msg = f"æ€§èƒ½å¯¹æ¯”æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}"
        print(f"âœ— {error_msg}")
        recorder.end_test("symbolic_execution_performance", False, error_msg)
        return False


if __name__ == "__main__":
    success, _ = run_conformance_test_with_semantics('summary', record_results=False)
    sys.exit(0 if success else 1) 